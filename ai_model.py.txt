# ai_model.py
import os
import joblib
import numpy as np
import pandas as pd

# supervised model (simple RF as starter)
from sklearn.ensemble import RandomForestClassifier

# stable-baselines3 for RL
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from env import TradingEnv

MODEL_DIR = "models"
SUP_MODEL_PATH = os.path.join(MODEL_DIR, "supervised.pkl")
RL_MODEL_PATH = os.path.join(MODEL_DIR, "ppo.zip")

def train_supervised(df: pd.DataFrame):
    # prepare features
    df = df.copy().dropna()
    df["future_return_5"] = df["close"].shift(-5) / df["close"] - 1.0
    df["label"] = (df["future_return_5"] > 0.01).astype(int)
    features = ["rsi","ema20","ema50","atr"]
    X = df[features].fillna(0).values
    y = df["label"].values
    model = RandomForestClassifier(n_estimators=200, n_jobs=-1)
    model.fit(X, y)
    os.makedirs(MODEL_DIR, exist_ok=True)
    joblib.dump(model, SUP_MODEL_PATH)
    return model

def load_supervised():
    if os.path.exists(SUP_MODEL_PATH):
        return joblib.load(SUP_MODEL_PATH)
    return None

def train_rl(df: pd.DataFrame, timesteps: int=10000):
    env = DummyVecEnv([lambda: TradingEnv(df, window_size=50)])
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=timesteps)
    os.makedirs(MODEL_DIR, exist_ok=True)
    model.save(RL_MODEL_PATH)
    return model

def load_rl():
    if os.path.exists(RL_MODEL_PATH):
        return PPO.load(RL_MODEL_PATH)
    return None
